9.22.14

The Data Science Workflow
1. Get Data
  2. Clean it
  3. Explore it
  4. Model it (machine learning) *relatively easy
  5. Interpret

What is Machine Learning?
	- Statistical Modeling
	- Computer Intelligence
	- Adaptive

software developer
	* follow steps to make computer do what you want
data scientist
	* write a black box that allows a computer to make decisions on its own

Types of machine learning problems
 supervised => making predictions
 unsupervised => discovering patterns
 	* running a clustering algorithm ( why did it cluster? )

 continuous => qualitative
 categorical => quantitative

 Linear Regression:
 	taking a set of features from an observation - creating numerical output
 	using one value to predict another

How to determine the right approach?
 the right approach is determined by the desired solution and
 the data availible

How to measure quality? (know if it works or not)

holdout - train model on one set, try it out on another set (data)

Classification algorithm
	- put data in space so that you can compare two observations
	- does not need to be x y axis, can handle n classifications

Prediction errors
	- Overfitting
		* works really well in training set but in real world it doesn't do as well
		* follows the changes too much, not following a real trend
	- Do not evaluate model agaisnt the set you have trained on
	- Bigger data set is always good... as long as you can handle it

Understading the topic of cross-validation is one of the most important things you
can get out of this class

********* stats
precision(p) = how many we got right / how many we got back
recall(r) = how many we got right / how many are really out there
f1 = 2pr / p + r

What's overfitting?
	- When your model fits too closely to your training sample to be generalized
	- Very important to avoid this

Σ |βi| this is called the L1-norm
Σ βi2 this is called the L2-norm

Why and how are we doing regularization?
	- minimize unnecessary params
	- prevent overfitting by explicity controlling the complexity

OLS: min(||y – xβ||2)
 - average of the squared differences

L1 regularization: min( || y – x β|| 2 + λ|| β|| )	
L2 regularization: min( || y – x β|| 2 + λ|| β|| 2 )
* a large value of complexity is probably going to hurt us

How do you solve an optimization problem?
	- calculus, set the first derivative equal to zero, second derivate negative

**lookup Dimensionality reduction

Checkout DataViz notebook for pandas graphing examples

Ridge is l2 regularization

normalization -- center and scaling