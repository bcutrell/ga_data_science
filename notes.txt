9.22.14

The Data Science Workflow
1. Get Data
  2. Clean it
  3. Explore it
  4. Model it (machine learning) *relatively easy
  5. Interpret

What is Machine Learning?
        - Statistical Modeling
        - Computer Intelligence
        - Adaptive

software developer
        * follow steps to make computer do what you want
data scientist
        * write a black box that allows a computer to make decisions on its own

Types of machine learning problems
 supervised => making predictions ( we know the class labels )
 unsupervised => discovering patterns
        * running a clustering algorithm ( why did it cluster? )

 continuous => qualitative
 categorical => quantitative

 Linear Regression:
        taking a set of features from an observation - creating numerical output
        using one value to predict another

# regression is not a classification problem

How to determine the right approach?
 the right approach is determined by the desired solution and
 the data availible

How to measure quality? (know if it works or not)

holdout - train model on one set, try it out on another set (data)

Classification algorithm
        - put data in space so that you can compare two observations
        - does not need to be x y axis, can handle n classifications

Prediction errors
        - Overfitting
                * works really well in training set but in real world it doesn't do as well
                * follows the changes too much, not following a real trend
        - Do not evaluate model agaisnt the set you have trained on
        - Bigger data set is always good... as long as you can handle it

Understading the topic of cross-validation is one of the most important things you
can get out of this class

********* stats
precision(p) = how many we got right / how many we got back
recall(r) = how many we got right / how many are really out there
f1 = 2pr / p + r

What's overfitting?
        - When your model fits too closely to your training sample to be generalized
        - Very important to avoid this

Σ |βi| this is called the L1-norm
Σ βi2 this is called the L2-norm

Why and how are we doing regularization?
        - minimize unnecessary params
        - prevent overfitting by explicity controlling the complexity

OLS: min(||y – xβ||2)
 - average of the squared differences

L1 regularization: min( || y – x β|| 2 + λ|| β|| )
L2 regularization: min( || y – x β|| 2 + λ|| β|| 2 )
* a large value of complexity is probably going to hurt us

How do you solve an optimization problem?
        - calculus, set the first derivative equal to zero, second derivate negative

**lookup Dimensionality reduction
PCA -- principal component analysis
SVD -- singular value decomposition

Checkout DataViz notebook for pandas graphing examples

Ridge is l2 regularization

normalization -- center and scaling

* logistic regression has the advantage of being a probablistic classifier
how is it related to linear regression?
  - doing a transformation to squash it on a 0,1 interval

For the binomial distribution, the confugate prior is the beta distribution

if you choose a good prior, youll get to reasonable probability sooner

likelihood function
  - can possibly require every single possible combination

Idenpendence => P(A|B) = P(A)
Dependence => P(A|B) = 

Matrix where each row is a document and the column is the feature count

*** When to split ****
Splits are determined by the purity condition
There are different measures of inpurity =>
entropy, gini, classification

*We are only going to use entropy
- entropy itself is a measure of impurity
We also want to quantify the change

* C4.5 is consistently the best decision tree algorithm
regularization => adds something to model that penalizes overfitting
- with decision trees this is done by using the gain ratio

*** When to stop ***
- pre running, setting a minimal threshold for gain

random forest
  - decide on a number of trees
  - have them all vote
  - the majority wins

min_df = word needs to occur at least once
spare_matrix improves performance

*************************************
10/22/14

Ensemble methods =>
 ensemble: combining multiple classifiers
 takes more time

types of problems
statistical: I dont have enough data
computational: find the best classifier

Types of Ensemble methods
Bagging:
  running a bunch of classifiers in parralell and then having
  them vote ( like a random forrest )
  overfitting is less likely with bagging

Boosting:
  iterative procedure that changes the samping distribution of
  training records at each iteration

  with any booster that is better than random you can keep
  iterating until you find something better but this has the
  possibility of overfitting

** Boosting is in series, bagging is in parallel

Anything where you can average the results in some way can be an ensemble method
*performance is a bigger deal than most people think

SVM is great for quickly updating the data,
points that are closest to the boundary lines are the support vectors

SVM is always worth a try but it is very hard to explain

KNN
Logistic Regression
Naive Bayes

unsupervised => no labeled answers

K means clustering is one of the best tools in the exploration phase

Greedy learner => decision trees, at each step we are doing local optimization
At every point, you have to recalculate centriods

Steps in developing Kmeans
1) Choose starting centroids
2) Assign each point to closest centroid
3) recalculate cluster centroids
4) repeat 2-3 until convergence

Cluster validation
- squared errors good for linear validation
- generalization
- taking a weighted average of silhouette coefficient
    - SSE versus number of clusters
    - average silhouette coefficient versus nmber of clusters in the data

*********************************
Recommendation systems
*********************************
11/3/14

*********************************
Networks
*********************************
11/19/14

Different types of networks
  - directed or undirected

number of connections => degree

