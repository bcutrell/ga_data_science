{
 "metadata": {
  "name": "",
  "signature": "sha256:89a27e03e0b4e821b305f3e77fb997c04cc0377b80d45ce2b2cd466579813acb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Naive Bayes Text Classification with Python NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import pandas and the NLTK:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function *prep_tweet_set* that takes a filepath of input tweet data in csv, reads it in with pandas, and outputs the [featureset, sentiment] combination that is necessary for the classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prep_tweet_set(filename):\n",
      "    prepped_tweets = []\n",
      "    # Read the csv file into a dataframe with Pandas, keeping only the sentiment and text columns\n",
      "    df = pd.read_csv(filename, names=['sentiment', 'timestamp', 'date', 'na', 'user', 'text'], index_col=False, usecols=[0,5])\n",
      "    # Use df.iterrows() to step through the dataframe and for each row do the following:\n",
      "    # 1. Tokenize the text using NLTK\n",
      "    # 2. Add to the prepped_tweets array an array that contains: (1) The tokenized text array and (2) The sentiment value\n",
      "    for row in df.iterrows():\n",
      "        tok_tweet = nltk.word_tokenize(row[1].text)\n",
      "        sentiment = row[1].sentiment\n",
      "        prepped_tweets.append([tok_tweet, sentiment])\n",
      "    return prepped_tweets\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prep the training set by calling your method above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_tweets = prep_tweet_set('/Users/pburkard88/Downloads/trainingandtestdata/training.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'prep_tweet_set' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-2448220ca0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_tweet_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/pburkard88/Downloads/trainingandtestdata/training.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'prep_tweet_set' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make a set of all the words in the training set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = set()\n",
      "for tweet in train_tweets:\n",
      "    all_words.update(tweet[0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function *extract_features* that takes a tweet observation and returns a dictionary of features.  Here a tweet observation is an array: [tweet tokens, sentiment]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features(tweet):\n",
      "    features = {}\n",
      "    # Use nltk.FreqDist(text) to extract the word frequencies for the tweet \n",
      "    tweet_wordfreq = nltk.FreqDist(tweet)\n",
      "    # For every word in the training set, set the feature value appropriately for that tweet\n",
      "    for word in all_words:\n",
      "        # If the word IS in the tweet...\n",
      "        if word in tweet_wordfreq:\n",
      "            features[word] = tweet_wordfreq[word]\n",
      "            #features[word] = 1\n",
      "        # If the word IS NOT in the tweet...\n",
      "        else:\n",
      "            features[word] = 0\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build your classifier against your training set via ***nltk.classify.apply_features*** and ***nltk.NaiveBayesClassifier.train***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_set = nltk.classify.apply_features(extract_features, train_tweets)\n",
      "classifier = nltk.NaiveBayesClassifier.train(training_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classifier._feature_probdist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test your classifier against the following tweet: \"I cannot believe the USA lost that game!!! I want to cry\".  Output the probabilities for each class, and the most influential features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = 'sad cry hate sleep miss dead'\n",
      "#test = 'I cannot believe the USA lost that game!!! I want to cry'\n",
      "prob = classifier.prob_classify(extract_features(test))\n",
      "print classifier.classify(extract_features(test))\n",
      "print prob.prob(4)\n",
      "# Demonstrate the 15 most important features via show_most_informative_features\n",
      "classifier.show_most_informative_features(15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4\n",
        "0.619578449168\n",
        "Most Informative Features\n",
        "                  thanks = 1                   4 : 0      =     13.4 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                    hate = 1                   0 : 4      =     10.2 : 1.0\n",
        "                     sad = 1                   0 : 4      =     10.1 : 1.0\n",
        "                  Thanks = 1                   4 : 0      =      8.6 : 1.0\n",
        "                   sleep = 1                   0 : 4      =      7.7 : 1.0\n",
        "                       . = 2                   0 : 4      =      7.0 : 1.0\n",
        "                   great = 1                   4 : 0      =      6.6 : 1.0\n",
        "                    Hope = 1                   4 : 0      =      6.3 : 1.0\n",
        "                     cry = 1                   0 : 4      =      6.3 : 1.0\n",
        "                    miss = 1                   0 : 4      =      5.7 : 1.0\n",
        "                    dead = 1                   0 : 4      =      5.7 : 1.0\n",
        "                   thats = 1                   4 : 0      =      5.7 : 1.0\n",
        "                     Yay = 1                   4 : 0      =      5.7 : 1.0\n",
        "                     bad = 1                   0 : 4      =      5.4 : 1.0\n",
        "                    Yeah = 1                   4 : 0      =      5.0 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Takes the set of all words (found above) and converts it into a dictionary of word --> vector index\n",
      "import numpy as np\n",
      "def gen_word_indexes(all_words):\n",
      "    return word_indexes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: Takes the list of training tweets and the set of all words and generates the matrices X and y for sklearn.naive_bayes.MultinomialNB\n",
      "def gen_data_matrices(train_tweets, all_words):\n",
      "    return X, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: Generate X, y using your function above and fit a MultinomialNB model to the data\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "#print train_tweets[2]\n",
      "#print clf.predict(X[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: Test/evaluate your model using cross_validation.StratifiedKFold with 3 folds and print out the performance\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}